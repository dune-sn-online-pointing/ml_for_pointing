# CNN Feature Interpretation for Electron Direction Reconstruction

**Date:** November 19, 2025  
**Generated by:** `cnn_feature_interpretation.py`  
**Model:** ED v58 (200k ES-only, 35.3° median)

---

## Overview

Understanding what features a CNN learns is challenging but crucial for:
1. **Validation**: Ensuring the model learns physics, not artifacts
2. **Improvement**: Identifying what information is underutilized
3. **Trust**: Building confidence in predictions for physics analysis

For **Boosted Decision Trees (BDTs)**, feature importance is straightforward - each feature has an explicit importance score. For **CNNs**, interpretation is harder because features are learned hierarchically in convolutional layers.

---

## What We CAN Analyze (Without Model Access)

### 1. **Energy-Dependent Performance** ✓

**Key Finding:** Performance improves dramatically with energy:

| Energy (MeV) | Median Error | Mean Cosine | Interpretation |
|--------------|--------------|-------------|----------------|
| 3            | 73.9°       | 0.166       | Very poor - insufficient track length |
| 5            | 50.9°       | 0.410       | Poor - short tracks, weak features |
| 7            | 42.3°       | 0.502       | Improving - Bragg peak developing |
| 10           | 37.4°       | 0.530       | Good - clear topology |
| 15           | 30.6°       | 0.578       | Excellent - strong features |
| 20+          | 23-28°      | 0.60-0.68   | **Best** - long tracks, clear asymmetry |

**Physics Interpretation:**
- **Low energy (<5 MeV):** Tracks too short, Bragg peak not well-developed
- **Medium energy (5-15 MeV):** Optimal range where features are strong
- **High energy (>20 MeV):** Best performance - long tracks with clear start/end distinction

**This tells us:** The model is successfully using **track-length-dependent features** (Bragg peak, scattering topology), which require sufficient track development.

### 2. **Error Mode Analysis** ✓

**Directional Categories (v58):**
- **Excellent (cosine > 0.9):** 37.87% - Model very confident, likely using clear Bragg peak
- **Good (0.7 < cosine ≤ 0.9):** 21.76% - Moderate features, some ambiguity
- **Mediocre (|cosine| < 0.7):** 33.34% - **Problem area** - insufficient directional information
- **Reversed (cosine < -0.7):** 7.04% - Directional ambiguity cases

**Key Insight:** 60% of events have excellent/good predictions, but 33% are in the mediocre zone. This suggests:
- Some events genuinely lack directional information (very short, scattered)
- Model architecture might not fully exploit available features
- Energy-dependent performance (see above) explains much of this

### 3. **Component-wise Prediction Bias** ✓

**X Component:** Mean residual = -0.055 (5.5% systematic bias)  
**Y Component:** Mean residual = +0.015 (1.5% bias)  
**Z Component:** Mean residual = +0.007 (0.7% bias)

**Interpretation:**
- **X direction shows systematic bias** - possibly due to detector geometry or clustering artifacts
- Y and Z are nearly unbiased
- RMS residuals are larger for Y (0.62) than Z (0.49), suggesting Y component is harder to predict

**This tells us:** The model may be learning some detector-specific biases in the X direction. This could be:
- Collection plane vs induction plane differences
- Wire orientation effects
- Clustering algorithm biases

---

## What We NEED Model Access For

### 1. **Gradient-Based Saliency Maps** (Requires Model + Images)

**Method:** Compute ∂(output)/∂(input pixel) for each pixel.

**What it shows:**
- Which pixels have the largest impact on the prediction
- Spatial regions that matter most (start? end? middle?)
- Whether model focuses on high-charge regions (Bragg peak)

**Implementation:**
```python
with tf.GradientTape() as tape:
    tape.watch(input_image)
    prediction = model(input_image)
    target_output = prediction[:, desired_component]
    
gradients = tape.gradient(target_output, input_image)
saliency = tf.abs(gradients)  # Absolute value for visualization
```

**Expected result:** 
- High saliency at track **endpoints** (Bragg peak region)
- Lower saliency in middle of track
- Possibly high saliency at **vertex** where track begins

### 2. **Integrated Gradients** (Requires Model + Images)

**Method:** Integrate gradients along path from baseline to input.

**Advantage over saliency:** More faithful attribution, handles saturation effects.

**What it shows:**
- Quantitative attribution of each pixel to the prediction
- More robust than simple gradients
- Can identify if model uses spurious correlations

**Reference:** Sundararajan et al. (2017) "Axiomatic Attribution for Deep Networks"

### 3. **Occlusion Analysis / Feature Ablation** (Requires Model + Images)

**Method:** 
1. Systematically mask out regions of input (e.g., 8x8 patches)
2. Re-run prediction for each masked version
3. Measure performance drop

**What it shows:**
- **Spatial importance map:** Which regions are critical
- **Plane importance:** Mask entire U, V, or X plane to see which matters most
- **Track segment importance:** Mask start vs end vs middle

**Expected results:**
- **X (collection) plane likely most important** - best signal/noise
- **Track ends more important than middle** - directional information
- **Start and end regions should show different importance** - asymmetry

**Implementation:**
```python
# Mask each plane individually
performance_all_planes = evaluate(model, images_full)
performance_no_U = evaluate(model, images_masked_U)
performance_no_V = evaluate(model, images_masked_V)
performance_no_X = evaluate(model, images_masked_X)

importance_U = performance_all_planes - performance_no_U
importance_V = performance_all_planes - performance_no_V
importance_X = performance_all_planes - performance_no_X
```

### 4. **Layer-wise Activation Analysis** (Requires Model)

**Method:** Visualize activations at each convolutional layer.

**What it shows:**
- **Low layers:** Edge detection, basic shapes
- **Middle layers:** Track segments, charge patterns
- **High layers:** Abstract concepts (direction, topology)

**Expected observations:**
- Early layers should detect charge clusters
- Middle layers should identify track-like structures
- Final layers should encode directional asymmetry

### 5. **Class Activation Maps (CAM/Grad-CAM)** (Requires Model + Images)

**Method:** Combine final conv layer activations with classification weights.

**What it shows:**
- **Heatmap** showing which spatial regions drive the prediction
- More interpretable than raw saliency
- Shows where model "looks" for each direction component

**Reference:** Zhou et al. (2016) "Learning Deep Features for Discriminative Localization"

---

## Techniques from BDT World (Not Directly Applicable)

### ❌ Feature Importance Scores
**Why not:** CNNs don't have explicit features - they learn them through conv layers.  
**Alternative:** Use gradient-based attribution or ablation studies.

### ❌ Partial Dependence Plots
**Why not:** No single "feature" to vary - each pixel is a feature.  
**Alternative:** Occlusion analysis serves similar purpose.

### ❌ SHAP Values
**Why not:** Combinatorially expensive for high-dimensional image inputs (128×32×3 = 12,288 dimensions).  
**Alternative:** Integrated gradients or LIME for images.

---

## What We CAN Infer (Even Without Deeper Analysis)

### Evidence Model Uses Physics-Based Features:

1. **Strong energy dependence (3 MeV: 74° → 20 MeV: 24°)**
   - If using spurious correlations: performance would be flat across energy
   - Observed trend matches physics expectation (longer tracks → better features)

2. **Cosine distribution strongly peaked at +1 (not -1)**
   - Model resolves directional ambiguity 13:1 ratio
   - Requires learning asymmetric features (Bragg peak, scattering)

3. **Component prediction accuracy varies (Z better than Y)**
   - Matches detector geometry: Z (drift) has best resolution
   - Y (vertical) more challenging due to wire spacing

4. **Systematic X-component bias (-5.5%)**
   - Likely detector artifact (collection vs induction planes)
   - Model learned this bias from training data

### Evidence Model Might Be Missing:

1. **33% of predictions in mediocre range**
   - Not using all available information?
   - Architecture limitation?
   - Some events genuinely ambiguous

2. **7% reversed predictions (cosine < -0.7)**
   - Directional ambiguity still present in some cases
   - Very short tracks or high scattering

3. **ES+CC mixing hurts performance (v60: 28.7° vs v58: 35.3°)**
   - Different event types have different optimal features
   - Single model tries to learn both → confusion
   - Suggests specialized models or event-type conditioning needed

---

## Recommendations for Future Work

### Immediate (No Additional Implementation):
1. ✅ **Energy-stratified training** - Separate models or weighting for different energy ranges
2. ✅ **ES-only models** - Stick with electron scatter (avoid CC confusion)
3. ✅ **Systematic bias correction** - Apply correction to X-component predictions

### Moderate Effort (Requires Validation Data Access):
4. **Plane ablation study** - Zero out each plane, measure importance
5. **Spatial occlusion analysis** - Mask track regions (start/middle/end)
6. **Best/worst case analysis** - Visualize what makes predictions succeed/fail

### Significant Effort (Requires Full Implementation):
7. **Grad-CAM visualization** - Generate heatmaps for many predictions
8. **Integrated gradients** - Robust feature attribution
9. **Attention mechanisms** - Let model explicitly show what it's looking at

---

## Conclusions

**What we know:**
- ✅ Model successfully learns energy-dependent, asymmetric features
- ✅ Performance aligns with physics expectations (Bragg peak, track length)
- ✅ Directional ambiguity mostly resolved (13:1 correct:reversed ratio)
- ⚠️ 33% of events have mediocre predictions - improvement opportunity
- ⚠️ Systematic X-component bias suggests detector artifacts learned

**What we need to confirm:**
- Does model focus on track endpoints (Bragg peak region)?
- Which plane (U, V, X) contributes most to direction?
- Are there spatial biases in what regions model considers?
- Is architecture fully exploiting available information?

**Best path forward:**
- Implement **plane ablation** study first (easiest, high impact)
- Add **Grad-CAM visualization** for interpretability
- Consider **attention mechanisms** in next model iteration

---

## Tools and Scripts

**Analysis tool:** `/afs/cern.ch/work/e/evilla/private/dune/refactor_ml/electron_direction/ana/cnn_feature_interpretation.py`

**Usage:**
```bash
python3 cnn_feature_interpretation.py <model_dir> -o output.pdf
```

**Current capabilities:**
- Energy-dependent performance analysis
- Error mode characterization  
- Component-wise prediction analysis
- Cosine similarity distributions

**Future additions needed:**
- Gradient-based saliency (requires model + images)
- Occlusion/ablation analysis (requires images)
- Activation visualization (requires model)
