#!/usr/bin/env python3
"""
Comprehensive Electron Direction Analysis
Generates a multi-page PDF with all analysis plots including:
- Angular error distributions and statistics
- Training history
- Component-wise correlations (X, Y, Z)
- Energy-dependent performance
- Best and worst predictions visualization
- Cosine similarity analysis
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.gridspec as gridspec
from scipy.stats import binned_statistic_2d
import json
import argparse
import os
import sys
from pathlib import Path

def load_results(results_dir):
    """Load results.json and predictions."""
    results_path = Path(results_dir) / 'results.json'
    pred_path = Path(results_dir) / 'val_predictions.npz'
    
    if not results_path.exists():
        raise FileNotFoundError(f"No results.json found in {results_dir}")
    
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    if pred_path.exists():
        predictions = np.load(pred_path)
    else:
        predictions = None
    
    return results, predictions


def plot_angular_error_analysis(predictions, results, fig):
    """Page 1: Comprehensive angular error analysis."""
    errors = predictions['angular_errors']
    
    # Create 2x2 grid
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: Error distribution histogram
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='steelblue')
    mean_err = results['angular_error_mean']
    median_err = results['angular_error_median']
    q68 = np.percentile(errors, 68)
    ax1.axvline(mean_err, color='r', linestyle='--', linewidth=2, label=f'Mean: {mean_err:.2f}¬∞')
    ax1.axvline(median_err, color='g', linestyle='--', linewidth=2, label=f'Median: {median_err:.2f}¬∞')
    ax1.axvline(q68, color='orange', linestyle='--', linewidth=2, label=f'68%: {q68:.2f}¬∞')
    ax1.set_xlabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # Plot 2: Cumulative distribution
    ax2 = fig.add_subplot(gs[0, 1])
    sorted_errors = np.sort(errors)
    cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
    ax2.plot(sorted_errors, cumulative, linewidth=2, color='navy')
    ax2.axvline(q68, color='orange', linestyle='--', linewidth=2, alpha=0.7, label=f'68%: {q68:.2f}¬∞')
    ax2.axhline(68, color='orange', linestyle='--', linewidth=2, alpha=0.7)
    ax2.set_xlabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Cumulative Percentage', fontsize=12, fontweight='bold')
    ax2.set_title('Cumulative Error Distribution', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)
    ax2.set_xlim(0, min(180, np.percentile(errors, 99)))
    
    # Plot 3: Cosine distribution with 68% quantile (replacing percentiles bar chart)
    ax3 = fig.add_subplot(gs[1, 0])
    
    # Calculate cosine similarity
    pred = predictions['predictions']
    true = predictions['true_directions']
    cosine_sim = np.sum(pred * true, axis=1)
    
    # Calculate 68% quantile from the top (best cosines)
    sorted_cosine = np.sort(cosine_sim)[::-1]  # Descending order
    idx_68 = int(0.68 * len(sorted_cosine))
    cosine_68 = sorted_cosine[idx_68]
    angle_68 = np.degrees(np.arccos(np.clip(cosine_68, -1, 1)))
    
    mean_cosine = np.mean(cosine_sim)
    
    # Plot histogram
    ax3.hist(cosine_sim, bins=80, alpha=0.7, edgecolor='black', color='steelblue', range=(-1, 1))
    ax3.axvline(0, color='red', linestyle='--', linewidth=2, label='cos=0 (90¬∞)', alpha=0.7)
    ax3.axvline(mean_cosine, color='blue', linestyle='--', linewidth=2, 
               label=f'Mean: {mean_cosine:.3f}', alpha=0.7)
    ax3.axvline(cosine_68, color='green', linestyle=':', linewidth=3,
               label=f'68%: {cosine_68:.3f} ({angle_68:.1f}¬∞)', alpha=0.9)
    
    ax3.set_xlabel('Cosine Similarity', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax3.set_title('Cosine Distribution (68% Quantile)', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=9)
    ax3.grid(alpha=0.3)
    
    # Plot 4: Statistics summary text
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.axis('off')
    
    stats_text = f"""ANGULAR ERROR STATS
    
Samples: {len(errors):,}
    
Mean:   {mean_err:.2f}¬∞
Median: {median_err:.2f}¬∞
Std:    {results['angular_error_std']:.2f}¬∞
    
25th: {results['angular_error_25th']:.2f}¬∞
50th: {median_err:.2f}¬∞
68th: {q68:.2f}¬∞
75th: {results['angular_error_75th']:.2f}¬∞
90th: {np.percentile(errors, 90):.2f}¬∞
95th: {np.percentile(errors, 95):.2f}¬∞
99th: {np.percentile(errors, 99):.2f}¬∞
    
Min: {np.min(errors):.2f}¬∞
Max: {np.max(errors):.2f}¬∞"""
    
    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, pad=0.6))


def plot_training_history(results, fig):
    """Page 2: Training history analysis."""
    history = results['history']
    
    # Filter out NaN values
    epochs_loss = [i+1 for i, x in enumerate(history['loss']) if not (isinstance(x, float) and np.isnan(x))]
    loss = [x for x in history['loss'] if not (isinstance(x, float) and np.isnan(x))]
    val_loss = [x for x in history['val_loss'] if not (isinstance(x, float) and np.isnan(x))]
    
    gs = gridspec.GridSpec(2, 1, figure=fig, hspace=0.3)
    
    # Plot 1: Loss curves
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(epochs_loss, loss, label='Training Loss', linewidth=2, marker='o', markersize=3)
    ax1.plot(epochs_loss, val_loss, label='Validation Loss', linewidth=2, marker='s', markersize=3)
    
    # Mark best epoch
    best_epoch = np.argmin(val_loss) + 1
    best_val_loss = np.min(val_loss)
    ax1.axvline(best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best epoch: {best_epoch}')
    ax1.scatter([best_epoch], [best_val_loss], color='red', s=100, zorder=5, marker='*')
    
    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax1.set_title('Training History - Loss', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # Plot 2: Learning rate schedule (if available)
    ax2 = fig.add_subplot(gs[1, 0])
    if 'learning_rate' in history:
        lr = [x for x in history['learning_rate'] if not (isinstance(x, float) and np.isnan(x))]
        epochs_lr = [i+1 for i, x in enumerate(history['learning_rate']) if not (isinstance(x, float) and np.isnan(x))]
        ax2.plot(epochs_lr, lr, linewidth=2, marker='o', markersize=3, color='green')
        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')
        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.grid(alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'Learning rate schedule not available', 
                ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        ax2.axis('off')


def plot_component_correlations(predictions, fig):
    """Page 3: X, Y, Z component correlation analysis."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.35, wspace=0.35)
    
    # Downsample for better visualization if too many points
    n_plot = min(5000, len(pred))
    if len(pred) > n_plot:
        idx = np.random.choice(len(pred), n_plot, replace=False)
        pred_plot = pred[idx]
        true_plot = true[idx]
        errors_plot = errors[idx]
    else:
        pred_plot = pred
        true_plot = true
        errors_plot = errors
    
    components = ['X', 'Y', 'Z']
    for i, comp in enumerate(components):
        # Scatter plot
        ax = fig.add_subplot(gs[0, i])
        
        # Color by angular error
        scatter = ax.scatter(true_plot[:, i], pred_plot[:, i], c=errors_plot, cmap='viridis', 
                           alpha=0.5, s=15, vmin=0, vmax=np.percentile(errors, 95))
        
        # Perfect prediction line
        ax.plot([-1, 1], [-1, 1], 'r--', linewidth=2.5, 
               label='Perfect', alpha=0.8, zorder=10)
        
        # Flipped line
        ax.plot([-1, 1], [1, -1], 'gray', linestyle=':', 
               linewidth=2, label='Flipped', alpha=0.6, zorder=10)
        
        ax.set_xlabel(f'True {comp}', fontsize=11, fontweight='bold')
        ax.set_ylabel(f'Predicted {comp}', fontsize=11, fontweight='bold')
        ax.set_title(f'{comp} Component Correlation', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9, loc='upper left')
        ax.grid(alpha=0.3)
        ax.set_xlim(-1.05, 1.05)
        ax.set_ylim(-1.05, 1.05)
        ax.set_aspect('equal')
        
        # Add colorbar
        if i == 2:
            cbar = plt.colorbar(scatter, ax=ax)
            cbar.set_label('Angular Error (¬∞)', fontsize=10, fontweight='bold')
        
        # Residual plot
        ax_res = fig.add_subplot(gs[1, i])
        residuals = pred_plot[:, i] - true_plot[:, i]
        ax_res.scatter(true_plot[:, i], residuals, c=errors_plot, cmap='viridis', 
                      alpha=0.5, s=15, vmin=0, vmax=np.percentile(errors, 95))
        ax_res.axhline(0, color='r', linestyle='--', linewidth=2, alpha=0.7)
        ax_res.set_xlabel(f'True {comp}', fontsize=11, fontweight='bold')
        ax_res.set_ylabel(f'Residual', fontsize=11, fontweight='bold')
        ax_res.set_title(f'{comp} Residuals', fontsize=12, fontweight='bold')
        ax_res.set_xlim(-1.05, 1.05)
        ax_res.grid(alpha=0.3)


def plot_energy_analysis(predictions, fig):
    """Page 4: Performance vs energy analysis."""
    if 'energies' not in predictions:
        # Create placeholder
        ax = fig.add_subplot(111)
        ax.text(0.5, 0.5, 'Energy data not available in predictions', 
               ha='center', va='center', fontsize=14)
        ax.axis('off')
        return
    
    energies = predictions['energies']
    errors = predictions['angular_errors']
    
    # Filter out invalid energies
    valid_mask = (energies > 0) & (energies < 1000)  # Reasonable energy range in MeV
    energies_valid = energies[valid_mask]
    errors_valid = errors[valid_mask]
    
    if len(energies_valid) == 0:
        ax = fig.add_subplot(111)
        ax.text(0.5, 0.5, 'No valid energy data available', 
               ha='center', va='center', fontsize=14)
        ax.axis('off')
        return
    
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: Error vs energy scatter
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.scatter(energies_valid, errors_valid, alpha=0.3, s=10)
    ax1.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error vs Energy', fontsize=14, fontweight='bold')
    ax1.grid(alpha=0.3)
    
    # Plot 2: Mean error in energy bins
    ax2 = fig.add_subplot(gs[0, 1])
    n_bins = 20
    energy_bins = np.linspace(energies_valid.min(), energies_valid.max(), n_bins + 1)
    bin_centers = (energy_bins[:-1] + energy_bins[1:]) / 2
    
    mean_errors = []
    median_errors = []
    std_errors = []
    
    for i in range(n_bins):
        mask = (energies_valid >= energy_bins[i]) & (energies_valid < energy_bins[i+1])
        if np.sum(mask) > 0:
            mean_errors.append(np.mean(errors_valid[mask]))
            median_errors.append(np.median(errors_valid[mask]))
            std_errors.append(np.std(errors_valid[mask]))
        else:
            mean_errors.append(np.nan)
            median_errors.append(np.nan)
            std_errors.append(np.nan)
    
    mean_errors = np.array(mean_errors)
    median_errors = np.array(median_errors)
    std_errors = np.array(std_errors)
    
    ax2.errorbar(bin_centers, mean_errors, yerr=std_errors, fmt='o-', 
                capsize=5, linewidth=2, markersize=6, label='Mean ¬± Std')
    ax2.plot(bin_centers, median_errors, 's--', linewidth=2, markersize=6, 
            label='Median', alpha=0.7)
    ax2.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_title('Mean Error vs Energy', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)
    
    # Plot 3: Energy distribution
    ax3 = fig.add_subplot(gs[1, 0])
    ax3.hist(energies_valid, bins=50, alpha=0.7, edgecolor='black', color='steelblue')
    ax3.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax3.set_title('Energy Distribution', fontsize=14, fontweight='bold')
    ax3.grid(alpha=0.3)
    
    # Plot 4: 68% containment vs energy
    ax4 = fig.add_subplot(gs[1, 1])
    q68_per_bin = []
    for i in range(n_bins):
        mask = (energies_valid >= energy_bins[i]) & (energies_valid < energy_bins[i+1])
        if np.sum(mask) > 10:  # Require at least 10 samples
            q68_per_bin.append(np.percentile(errors_valid[mask], 68))
        else:
            q68_per_bin.append(np.nan)
    
    ax4.plot(bin_centers, q68_per_bin, 'o-', linewidth=2, markersize=6, color='orange')
    ax4.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax4.set_ylabel('68% Containment (degrees)', fontsize=12, fontweight='bold')
    ax4.set_title('Precision vs Energy', fontsize=14, fontweight='bold')
    ax4.grid(alpha=0.3)


def plot_cosine_similarity_analysis(predictions, fig):
    """Page 5: Cosine similarity analysis with component breakdown."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    # Calculate cosine similarity
    cosine_sim = np.sum(pred * true, axis=1)
    
    # Statistics
    q68_cosine = np.percentile(np.sort(cosine_sim)[::-1], 32)  # 68% from top
    mean_cosine = np.mean(cosine_sim)
    n_positive = np.sum(cosine_sim > 0)
    n_negative = np.sum(cosine_sim < 0)
    n_flipped = np.sum(cosine_sim < -0.5)
    
    gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.35, wspace=0.35)
    
    # Plot 1: Cosine distribution
    ax1 = fig.add_subplot(gs[0, :2])
    ax1.hist(cosine_sim, bins=100, alpha=0.7, edgecolor='black', color='steelblue', range=(-1, 1))
    ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='cos=0 (90¬∞)', alpha=0.7)
    ax1.axvline(mean_cosine, color='blue', linestyle='--', linewidth=2, 
               label=f'Mean: {mean_cosine:.3f}', alpha=0.7)
    ax1.axvline(q68_cosine, color='green', linestyle=':', linewidth=3,
               label=f'68% quantile: {q68_cosine:.3f}', alpha=0.9)
    ax1.set_xlabel('Cosine Similarity (True ¬∑ Predicted)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Count', fontsize=12, fontweight='bold')
    ax1.set_title('Cosine Similarity Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # Plot 2: Statistics text
    ax2 = fig.add_subplot(gs[0, 2])
    ax2.axis('off')
    stats_text = f"""COSINE STATS

Total: {len(cosine_sim):,}

Positive: {n_positive:,}
({100*n_positive/len(cosine_sim):.1f}%)

Negative: {n_negative:,}
({100*n_negative/len(cosine_sim):.1f}%)

Flipped: {n_flipped:,}
({100*n_flipped/len(cosine_sim):.1f}%)

Mean: {mean_cosine:.3f}
68%:  {q68_cosine:.3f}"""
    ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, pad=0.6))
    
    # Plot 3: Cosine vs angular error
    ax3 = fig.add_subplot(gs[1, 0])
    scatter = ax3.scatter(cosine_sim, errors, alpha=0.3, s=10, c=errors, cmap='viridis',
                         vmin=0, vmax=np.percentile(errors, 95))
    ax3.set_xlabel('Cosine Similarity', fontsize=11, fontweight='bold')
    ax3.set_ylabel('Angular Error (degrees)', fontsize=11, fontweight='bold')
    ax3.set_title('Cosine vs Angular Error', fontsize=12, fontweight='bold')
    ax3.grid(alpha=0.3)
    ax3.axvline(0, color='red', linestyle='--', alpha=0.5)
    ax3.axhline(90, color='red', linestyle='--', alpha=0.5)
    
    # Plot 4 & 5: Good vs bad predictions spatial distribution (2D projection)
    good_mask = errors < 45
    bad_mask = errors > 135
    
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.scatter(cosine_sim[good_mask], errors[good_mask], alpha=0.5, s=20, 
               color='green', label=f'Good (<45¬∞): {np.sum(good_mask)}')
    ax4.scatter(cosine_sim[bad_mask], errors[bad_mask], alpha=0.5, s=20, 
               color='red', label=f'Bad (>135¬∞): {np.sum(bad_mask)}')
    ax4.set_xlabel('Cosine Similarity', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Angular Error (degrees)', fontsize=11, fontweight='bold')
    ax4.set_title('Good vs Bad Predictions', fontsize=12, fontweight='bold')
    ax4.legend(fontsize=9)
    ax4.grid(alpha=0.3)
    
    # Plot 6: 2D cosine-error density
    ax5 = fig.add_subplot(gs[1, 2])
    h = ax5.hist2d(cosine_sim, errors, bins=50, cmap='viridis', 
                  range=[[-1, 1], [0, 180]])
    plt.colorbar(h[3], ax=ax5, label='Count')
    ax5.set_xlabel('Cosine Similarity', fontsize=11, fontweight='bold')
    ax5.set_ylabel('Angular Error (degrees)', fontsize=11, fontweight='bold')
    ax5.set_title('2D Distribution', fontsize=12, fontweight='bold')


def plot_best_worst_predictions(predictions, fig):
    """Page 6: Visualization of best and worst predictions."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    # Find best and worst
    best_idx = np.argmin(errors)
    worst_idx = np.argmax(errors)
    
    # Find some representative mediocre ones
    median_error = np.median(errors)
    median_idx = np.argmin(np.abs(errors - median_error))
    
    gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.4, wspace=0.4)
    
    examples = [
        (best_idx, 'Best Prediction', 0),
        (median_idx, 'Median Prediction', 1),
        (worst_idx, 'Worst Prediction', 2)
    ]
    
    for idx, title, row in examples:
        # 3D visualization
        ax_3d = fig.add_subplot(gs[row, 0], projection='3d')
        
        # Origin
        ax_3d.scatter([0], [0], [0], c='black', s=100, marker='o', label='Origin')
        
        # True direction
        ax_3d.quiver(0, 0, 0, true[idx, 0], true[idx, 1], true[idx, 2],
                    color='blue', arrow_length_ratio=0.3, linewidth=3, 
                    label='True', alpha=0.8)
        
        # Predicted direction
        ax_3d.quiver(0, 0, 0, pred[idx, 0], pred[idx, 1], pred[idx, 2],
                    color='red', arrow_length_ratio=0.3, linewidth=3,
                    label='Predicted', alpha=0.8)
        
        ax_3d.set_xlabel('X', fontweight='bold')
        ax_3d.set_ylabel('Y', fontweight='bold')
        ax_3d.set_zlabel('Z', fontweight='bold')
        ax_3d.set_title(f'{title}\nError: {errors[idx]:.2f}¬∞', fontsize=11, fontweight='bold')
        ax_3d.legend(fontsize=8)
        
        # Set equal aspect ratio
        max_range = 1.0
        ax_3d.set_xlim([-max_range, max_range])
        ax_3d.set_ylim([-max_range, max_range])
        ax_3d.set_zlim([-max_range, max_range])
        
        # Component comparison bar chart
        ax_bar = fig.add_subplot(gs[row, 1])
        components = ['X', 'Y', 'Z']
        x_pos = np.arange(len(components))
        width = 0.35
        
        ax_bar.bar(x_pos - width/2, true[idx], width, label='True', alpha=0.8, color='blue')
        ax_bar.bar(x_pos + width/2, pred[idx], width, label='Predicted', alpha=0.8, color='red')
        ax_bar.set_xticks(x_pos)
        ax_bar.set_xticklabels(components)
        ax_bar.set_ylabel('Component Value', fontweight='bold')
        ax_bar.set_title('Component Comparison', fontsize=11, fontweight='bold')
        ax_bar.legend(fontsize=8)
        ax_bar.grid(axis='y', alpha=0.3)
        ax_bar.axhline(0, color='black', linewidth=0.5)
        ax_bar.set_ylim([-1.2, 1.2])
        
        # Info text
        ax_info = fig.add_subplot(gs[row, 2])
        ax_info.axis('off')
        
        cosine = np.sum(pred[idx] * true[idx])
        info_text = f"""DETAILS

Ang Err: {errors[idx]:.2f}¬∞
Cosine:  {cosine:.4f}

True Direction:
  X: {true[idx, 0]:+.3f}
  Y: {true[idx, 1]:+.3f}
  Z: {true[idx, 2]:+.3f}

Predicted:
  X: {pred[idx, 0]:+.3f}
  Y: {pred[idx, 1]:+.3f}
  Z: {pred[idx, 2]:+.3f}

Residuals:
  ŒîX: {pred[idx, 0] - true[idx, 0]:+.3f}
  ŒîY: {pred[idx, 1] - true[idx, 1]:+.3f}
  ŒîZ: {pred[idx, 2] - true[idx, 2]:+.3f}"""
        
        ax_info.text(0.05, 0.95, info_text, transform=ax_info.transAxes, fontsize=8.5,
                    verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8, pad=0.5))


def plot_angular_error_vs_true_angles(predictions, fig):
    """Page 7: Angular error dependence on true theta and phi angles."""
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    # Convert to spherical coordinates
    # theta: polar angle (0 to 180¬∞)
    # phi: azimuthal angle (-180 to 180¬∞)
    theta_true = np.degrees(np.arccos(np.clip(true[:, 2], -1, 1)))
    phi_true = np.degrees(np.arctan2(true[:, 1], true[:, 0]))
    
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: 2D histogram - Error vs theta
    ax1 = fig.add_subplot(gs[0, 0])
    
    # Create 2D histogram: bins for theta (x-axis) and error (y-axis)
    theta_bins = np.linspace(0, 180, 50)
    error_bins = np.linspace(0, np.percentile(errors, 99), 50)
    
    h1 = ax1.hist2d(theta_true, errors, bins=[theta_bins, error_bins], 
                    cmap='YlOrRd', cmin=1)
    cbar1 = plt.colorbar(h1[3], ax=ax1, label='Count')
    
    ax1.set_xlabel('True Œ∏ (degrees)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error vs True Œ∏', fontsize=14, fontweight='bold')
    ax1.grid(alpha=0.3, linestyle=':', linewidth=0.5)
    
    # Plot 2: 2D histogram - Error vs phi
    ax2 = fig.add_subplot(gs[0, 1])
    
    phi_bins = np.linspace(-180, 180, 50)
    
    h2 = ax2.hist2d(phi_true, errors, bins=[phi_bins, error_bins], 
                    cmap='YlOrRd', cmin=1)
    cbar2 = plt.colorbar(h2[3], ax=ax2, label='Count')
    
    ax2.set_xlabel('True œÜ (degrees)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_title('Angular Error vs True œÜ', fontsize=14, fontweight='bold')
    ax2.grid(alpha=0.3, linestyle=':', linewidth=0.5)
    
    # Plot 3: 2D histogram - mean error in theta-phi space
    ax3 = fig.add_subplot(gs[1, :])
    
    # Calculate mean error per bin
    theta_bins_3 = np.linspace(0, 180, 45)
    phi_bins_3 = np.linspace(-180, 180, 45)
    
    # Use binned_statistic_2d to compute mean
    from scipy.stats import binned_statistic_2d
    mean_errors, theta_edges, phi_edges, _ = binned_statistic_2d(
        theta_true, phi_true, errors, statistic='mean', 
        bins=[theta_bins_3, phi_bins_3])
    
    # Plot as image
    im = ax3.imshow(mean_errors.T, origin='lower', cmap='hot', aspect='auto',
                   extent=[0, 180, -180, 180], vmin=0, vmax=np.percentile(errors, 90))
    cbar3 = plt.colorbar(im, ax=ax3, label='Mean Angular Error (¬∞)')
    
    ax3.set_xlabel('True Œ∏ (degrees)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('True œÜ (degrees)', fontsize=12, fontweight='bold')
    ax3.set_title('Mean Angular Error in (Œ∏, œÜ) Space', fontsize=14, fontweight='bold')


def generate_comprehensive_analysis(results_dir, output_pdf=None):
    """Generate comprehensive multi-page PDF analysis."""
    
    print("\n" + "="*80)
    print("COMPREHENSIVE ELECTRON DIRECTION ANALYSIS")
    print("="*80 + "\n")
    
    # Load results
    print("üìä Loading results...")
    results, predictions = load_results(results_dir)
    
    if predictions is None:
        print("‚ùå No predictions found. Run training with save_predictions=True")
        return
    
    model_name = results['config']['model']['name']
    print(f"‚úì Model: {model_name}")
    print(f"‚úì Predictions: {len(predictions['angular_errors']):,} samples")
    
    # Determine output path
    if output_pdf is None:
        output_pdf = Path(results_dir) / f"{model_name}_comprehensive_analysis.pdf"
    else:
        output_pdf = Path(output_pdf)
    
    print(f"‚úì Output: {output_pdf}\n")
    
    # Create multi-page PDF
    with PdfPages(output_pdf) as pdf:
        # Page 1: Angular error analysis
        print("üìà Generating page 1/7: Angular Error Analysis...")
        fig = plt.figure(figsize=(14, 10))
        plot_angular_error_analysis(predictions, results, fig)
        fig.suptitle(f'{model_name.upper()} - Angular Error Analysis', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 2: Training history
        print("üìà Generating page 2/7: Training History...")
        fig = plt.figure(figsize=(14, 10))
        plot_training_history(results, fig)
        fig.suptitle(f'{model_name.upper()} - Training History', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 3: Component correlations
        print("üìà Generating page 3/7: Component Correlations...")
        fig = plt.figure(figsize=(16, 10))
        plot_component_correlations(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Component Correlations', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 4: Energy analysis
        print("üìà Generating page 4/7: Energy-Dependent Performance...")
        fig = plt.figure(figsize=(14, 10))
        plot_energy_analysis(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Energy-Dependent Performance', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 5: Cosine similarity
        print("üìà Generating page 5/7: Cosine Similarity Analysis...")
        fig = plt.figure(figsize=(16, 10))
        plot_cosine_similarity_analysis(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Cosine Similarity Analysis', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 6: Best/worst predictions
        print("üìà Generating page 6/7: Best and Worst Predictions...")
        fig = plt.figure(figsize=(16, 12))
        plot_best_worst_predictions(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Best and Worst Predictions', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 7: Angular error vs true angles
        print("üìà Generating page 7/7: Error vs True Angles...")
        fig = plt.figure(figsize=(14, 10))
        plot_angular_error_vs_true_angles(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Error Dependence on True Angles', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
    
    print(f"\n‚úÖ Analysis complete! Saved to: {output_pdf}")
    print("="*80 + "\n")
    
    return output_pdf


def main():
    parser = argparse.ArgumentParser(
        description='Generate comprehensive ED analysis PDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s /path/to/results/dir
  %(prog)s /path/to/results/dir -o custom_name.pdf
        """
    )
    parser.add_argument('results_dir', help='Path to model results directory')
    parser.add_argument('-o', '--output', help='Output PDF path (optional)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.results_dir):
        print(f"‚ùå Directory not found: {args.results_dir}")
        return 1
    
    try:
        generate_comprehensive_analysis(args.results_dir, args.output)
        return 0
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
