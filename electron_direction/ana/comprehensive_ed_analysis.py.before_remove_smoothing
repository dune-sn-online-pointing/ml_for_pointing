#!/usr/bin/env python3
"""
Comprehensive Electron Direction Analysis
Generates a multi-page PDF with all analysis plots including:
- Angular error distributions and statistics
- Training history
- Component-wise correlations (X, Y, Z)
- Energy-dependent performance
- Best and worst predictions visualization
- Cosine similarity analysis
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.gridspec as gridspec
from scipy.stats import binned_statistic_2d, gaussian_kde
import json
import argparse
import os
import sys
from pathlib import Path

def load_results(results_dir):
    """Load results.json and predictions."""
    results_path = Path(results_dir) / 'results.json'
    pred_path = Path(results_dir) / 'val_predictions.npz'
    
    if not results_path.exists():
        raise FileNotFoundError(f"No results.json found in {results_dir}")
    
    with open(results_path, 'r') as f:
        results = json.load(f)
    
    if pred_path.exists():
        predictions = np.load(pred_path)
        
        # Calculate cosine similarity if not present
        if 'cosine_similarity' not in predictions:
            pred_dir = predictions['predictions']
            true_dir = predictions['true_directions']
            cosine_sim = np.sum(pred_dir * true_dir, axis=1)
            # Add to predictions dict (convert to dict first)
            pred_dict = {k: predictions[k] for k in predictions.keys()}
            pred_dict['cosine_similarity'] = cosine_sim
            # Convert back to namespace-like object
            class PredDict(dict):
                def __getitem__(self, key):
                    return super().__getitem__(key)
            predictions = PredDict(pred_dict)
    else:
        predictions = None
    
    return results, predictions


def plot_angular_error_analysis(predictions, results, fig):
    """Page 1: Comprehensive angular error analysis."""
    errors = predictions['angular_errors']
    
    # Create 2x2 grid
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: Error distribution histogram
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.hist(errors, bins=50, alpha=0.7, edgecolor='black', color='steelblue')
    mean_err = results['angular_error_mean']
    median_err = results['angular_error_median']
    q68 = np.percentile(errors, 68)
    ax1.axvline(mean_err, color='r', linestyle='--', linewidth=2, label=f'Mean: {mean_err:.2f}Â°')
    ax1.axvline(median_err, color='g', linestyle='--', linewidth=2, label=f'Median: {median_err:.2f}Â°')
    ax1.axvline(q68, color='orange', linestyle='--', linewidth=2, label=f'68%: {q68:.2f}Â°')
    ax1.set_xlabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # Plot 2: Cumulative distribution
    ax2 = fig.add_subplot(gs[0, 1])
    sorted_errors = np.sort(errors)
    cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100
    ax2.plot(sorted_errors, cumulative, linewidth=2, color='navy')
    ax2.axvline(q68, color='orange', linestyle='--', linewidth=2, alpha=0.7, label=f'68%: {q68:.2f}Â°')
    ax2.axhline(68, color='orange', linestyle='--', linewidth=2, alpha=0.7)
    ax2.set_xlabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Cumulative Percentage', fontsize=12, fontweight='bold')
    ax2.set_title('Cumulative Error Distribution', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)
    ax2.set_xlim(0, min(180, np.percentile(errors, 99)))
    
    # Plot 3: Cosine distribution with 68% quantile (replacing percentiles bar chart)
    ax3 = fig.add_subplot(gs[1, 0])
    
    # Calculate cosine similarity
    pred = predictions['predictions']
    true = predictions['true_directions']
    cosine_sim = np.sum(pred * true, axis=1)
    
    # Calculate 68% quantile from the top (best cosines)
    sorted_cosine = np.sort(cosine_sim)[::-1]  # Descending order
    idx_68 = int(0.68 * len(sorted_cosine))
    cosine_68 = sorted_cosine[idx_68]
    angle_68 = np.degrees(np.arccos(np.clip(cosine_68, -1, 1)))
    
    mean_cosine = np.mean(cosine_sim)
    
    # Plot histogram
    ax3.hist(cosine_sim, bins=80, alpha=0.7, edgecolor='black', color='steelblue', range=(-1, 1))
    ax3.axvline(0, color='red', linestyle='--', linewidth=2, label='cos=0 (90Â°)', alpha=0.7)
    ax3.axvline(mean_cosine, color='blue', linestyle='--', linewidth=2, 
               label=f'Mean: {mean_cosine:.3f}', alpha=0.7)
    ax3.axvline(cosine_68, color='green', linestyle=':', linewidth=3,
               label=f'68%: {cosine_68:.3f} ({angle_68:.1f}Â°)', alpha=0.9)
    
    ax3.set_xlabel('Cosine Similarity', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax3.set_title('Cosine Distribution (68% Quantile)', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=9)
    ax3.grid(alpha=0.3)
    
    # Plot 4: Statistics summary text
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.axis('off')
    
    stats_text = f"""ANGULAR ERROR STATS
    
Samples: {len(errors):,}
    
Mean:   {mean_err:.2f}Â°
Median: {median_err:.2f}Â°
Std:    {results['angular_error_std']:.2f}Â°
    
25th: {results['angular_error_25th']:.2f}Â°
50th: {median_err:.2f}Â°
68th: {q68:.2f}Â°
75th: {results['angular_error_75th']:.2f}Â°
90th: {np.percentile(errors, 90):.2f}Â°
95th: {np.percentile(errors, 95):.2f}Â°
99th: {np.percentile(errors, 99):.2f}Â°
    
Min: {np.min(errors):.2f}Â°
Max: {np.max(errors):.2f}Â°"""
    
    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, pad=0.6))


def plot_training_history(results, fig):
    """Page 2: Training history analysis."""
    history = results['history']
    
    # Filter out NaN values
    epochs_loss = [i+1 for i, x in enumerate(history['loss']) if not (isinstance(x, float) and np.isnan(x))]
    loss = [x for x in history['loss'] if not (isinstance(x, float) and np.isnan(x))]
    val_loss = [x for x in history['val_loss'] if not (isinstance(x, float) and np.isnan(x))]
    
    gs = gridspec.GridSpec(2, 1, figure=fig, hspace=0.3)
    
    # Plot 1: Loss curves
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(epochs_loss, loss, label='Training Loss', linewidth=2, marker='o', markersize=3)
    ax1.plot(epochs_loss, val_loss, label='Validation Loss', linewidth=2, marker='s', markersize=3)
    
    # Mark best epoch
    best_epoch = np.argmin(val_loss) + 1
    best_val_loss = np.min(val_loss)
    ax1.axvline(best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best epoch: {best_epoch}')
    ax1.scatter([best_epoch], [best_val_loss], color='red', s=100, zorder=5, marker='*')
    
    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax1.set_title('Training History - Loss', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)
    
    # Plot 2: Learning rate schedule (if available)
    ax2 = fig.add_subplot(gs[1, 0])
    if 'learning_rate' in history:
        lr = [x for x in history['learning_rate'] if not (isinstance(x, float) and np.isnan(x))]
        epochs_lr = [i+1 for i, x in enumerate(history['learning_rate']) if not (isinstance(x, float) and np.isnan(x))]
        ax2.plot(epochs_lr, lr, linewidth=2, marker='o', markersize=3, color='green')
        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')
        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.grid(alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'Learning rate schedule not available', 
                ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        ax2.axis('off')


def _plot_hist2d(ax, x, y, x_range, y_range, bins=60, cmap='YlOrRd'):
    """Utility to render a log-friendly 2D histogram with colorbar."""
    h, xedges, yedges = np.histogram2d(x, y, bins=bins, range=[x_range, y_range])
    mesh = ax.pcolormesh(xedges, yedges, h.T, cmap=cmap)
    return mesh


def plot_component_correlations(predictions, fig):
    """Page 3: Component-wise correlation analysis using 2D histograms."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    
    gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.35, wspace=0.35)
    components = ['X', 'Y', 'Z']
    
    for idx, comp in enumerate(components):
        ax = fig.add_subplot(gs[0, idx])
        mesh = _plot_hist2d(ax, true[:, idx], pred[:, idx], [-1.05, 1.05], [-1.05, 1.05])
        ax.plot([-1, 1], [-1, 1], 'r--', linewidth=2.0, label='Perfect', alpha=0.8)
        ax.plot([-1, 1], [1, -1], 'gray', linestyle=':', linewidth=1.5, label='Flipped', alpha=0.6)
        ax.set_xlabel(f'True {comp}', fontsize=11, fontweight='bold')
        ax.set_ylabel(f'Predicted {comp}', fontsize=11, fontweight='bold')
        ax.set_title(f'{comp} Component Correlation', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9, loc='upper left')
        ax.set_xlim(-1.05, 1.05)
        ax.set_ylim(-1.05, 1.05)
        ax.set_aspect('equal')
        ax.grid(alpha=0.15, linestyle=':')
        if idx == 2:
            cbar = plt.colorbar(mesh, ax=ax)
            cbar.set_label('Count', fontsize=10, fontweight='bold')
        
        ax_res = fig.add_subplot(gs[1, idx])
        residuals = pred[:, idx] - true[:, idx]
        mesh_res = _plot_hist2d(ax_res, true[:, idx], residuals, [-1.05, 1.05], [-2, 2], bins=[60, 60])
        ax_res.axhline(0, color='r', linestyle='--', linewidth=1.5, alpha=0.7)
        ax_res.set_xlabel(f'True {comp}', fontsize=11, fontweight='bold')
        ax_res.set_ylabel('Residual', fontsize=11, fontweight='bold')
        ax_res.set_title(f'{comp} Residuals', fontsize=12, fontweight='bold')
        ax_res.set_xlim(-1.05, 1.05)
        ax_res.grid(alpha=0.15, linestyle=':')
        if idx == 2:
            cbar_res = plt.colorbar(mesh_res, ax=ax_res)
            cbar_res.set_label('Count', fontsize=10, fontweight='bold')



def plot_energy_analysis(predictions, fig):
    """Page 4: Performance vs energy analysis."""
    if 'energies' not in predictions:
        # Create placeholder
        ax = fig.add_subplot(111)
        ax.text(0.5, 0.5, 'Energy data not available in predictions', 
               ha='center', va='center', fontsize=14)
        ax.axis('off')
        return
    
    energies = predictions['energies']
    errors = predictions['angular_errors']
    
    # Filter out invalid energies
    valid_mask = (energies > 0) & (energies < 1000)  # Reasonable energy range in MeV
    energies_valid = energies[valid_mask]
    errors_valid = errors[valid_mask]
    
    if len(energies_valid) == 0:
        ax = fig.add_subplot(111)
        ax.text(0.5, 0.5, 'No valid energy data available', 
               ha='center', va='center', fontsize=14)
        ax.axis('off')
        return
    
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: Error vs energy 2D histogram
    ax1 = fig.add_subplot(gs[0, 0])
    energy_bins_2d = np.linspace(energies_valid.min(), energies_valid.max(), 60)
    error_bins_2d = np.linspace(0, np.percentile(errors_valid, 99), 60)
    hist, xedges, yedges = np.histogram2d(energies_valid, errors_valid, bins=[energy_bins_2d, error_bins_2d])
    mesh = ax1.pcolormesh(xedges, yedges, hist.T, cmap='YlOrRd')
    plt.colorbar(mesh, ax=ax1, label='Count')
    ax1.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error vs Energy (2D Histogram)', fontsize=14, fontweight='bold')
    ax1.grid(alpha=0.15)
    
    # Plot 2: Mean error in energy bins
    ax2 = fig.add_subplot(gs[0, 1])
    n_bins = 20
    energy_bins = np.linspace(energies_valid.min(), energies_valid.max(), n_bins + 1)
    bin_centers = (energy_bins[:-1] + energy_bins[1:]) / 2
    
    mean_errors = []
    median_errors = []
    std_errors = []
    
    for i in range(n_bins):
        mask = (energies_valid >= energy_bins[i]) & (energies_valid < energy_bins[i+1])
        if np.sum(mask) > 0:
            mean_errors.append(np.mean(errors_valid[mask]))
            median_errors.append(np.median(errors_valid[mask]))
            std_errors.append(np.std(errors_valid[mask]))
        else:
            mean_errors.append(np.nan)
            median_errors.append(np.nan)
            std_errors.append(np.nan)
    
    mean_errors = np.array(mean_errors)
    median_errors = np.array(median_errors)
    std_errors = np.array(std_errors)
    
    ax2.errorbar(bin_centers, mean_errors, yerr=std_errors, fmt='o-', 
                capsize=5, linewidth=2, markersize=6, label='Mean Â± Std')
    ax2.plot(bin_centers, median_errors, 's--', linewidth=2, markersize=6, 
            label='Median', alpha=0.7)
    ax2.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_title('Mean Error vs Energy', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)
    
    # Plot 3: Energy distribution
    ax3 = fig.add_subplot(gs[1, 0])
    ax3.hist(energies_valid, bins=50, alpha=0.7, edgecolor='black', color='steelblue')
    ax3.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')
    ax3.set_title('Energy Distribution', fontsize=14, fontweight='bold')
    ax3.grid(alpha=0.3)
    
    # Plot 4: 68% containment vs energy
    ax4 = fig.add_subplot(gs[1, 1])
    q68_per_bin = []
    for i in range(n_bins):
        mask = (energies_valid >= energy_bins[i]) & (energies_valid < energy_bins[i+1])
        if np.sum(mask) > 10:  # Require at least 10 samples
            q68_per_bin.append(np.percentile(errors_valid[mask], 68))
        else:
            q68_per_bin.append(np.nan)
    
    ax4.plot(bin_centers, q68_per_bin, 'o-', linewidth=2, markersize=6, color='orange')
    ax4.set_xlabel('Particle Energy (MeV)', fontsize=12, fontweight='bold')
    ax4.set_ylabel('68% Containment (degrees)', fontsize=12, fontweight='bold')
    ax4.set_title('Precision vs Energy', fontsize=14, fontweight='bold')
    ax4.grid(alpha=0.3)


def plot_cosine_similarity_analysis(predictions, fig):
    """Page 5: Cosine similarity analysis with streamlined visuals."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    
    cosine_sim = np.sum(pred * true, axis=1)
    q68_cosine = np.percentile(np.sort(cosine_sim)[::-1], 32)
    mean_cosine = np.mean(cosine_sim)
    n_positive = np.sum(cosine_sim > 0)
    n_negative = np.sum(cosine_sim < 0)
    n_flipped = np.sum(cosine_sim < -0.5)
    
    gs = gridspec.GridSpec(1, 2, figure=fig, wspace=0.25)
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.hist(cosine_sim, bins=100, alpha=0.75, edgecolor='black', color='steelblue', range=(-1, 1))
    ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='cos=0 (90Â°)', alpha=0.7)
    ax1.axvline(mean_cosine, color='blue', linestyle='--', linewidth=2, label=f'Mean: {mean_cosine:.3f}', alpha=0.7)
    ax1.axvline(q68_cosine, color='green', linestyle=':', linewidth=3, label=f'68% quantile: {q68_cosine:.3f}', alpha=0.9)
    ax1.set_xlabel('Cosine Similarity (True Â· Predicted)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Count', fontsize=12, fontweight='bold')
    ax1.set_title('Cosine Similarity Distribution', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.25)
    
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.axis('off')
    stats_text = f"""COSINE STATS

Total: {len(cosine_sim):,}

Positive: {n_positive:,} ({100*n_positive/len(cosine_sim):.1f}%)
Negative: {n_negative:,} ({100*n_negative/len(cosine_sim):.1f}%)
Flipped (<-0.5): {n_flipped:,} ({100*n_flipped/len(cosine_sim):.1f}%)

Mean: {mean_cosine:.3f}
68%:  {q68_cosine:.3f}"""
    ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes, fontsize=11,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.85, pad=0.7))


def plot_best_worst_predictions(predictions, fig):
    """Page 6: Visualization of best and worst predictions."""
    pred = predictions['predictions']
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    # Find best and worst
    best_idx = np.argmin(errors)
    worst_idx = np.argmax(errors)
    
    # Find some representative mediocre ones
    median_error = np.median(errors)
    median_idx = np.argmin(np.abs(errors - median_error))
    
    gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.4, wspace=0.4)
    
    examples = [
        (best_idx, 'Best Prediction', 0),
        (median_idx, 'Median Prediction', 1),
        (worst_idx, 'Worst Prediction', 2)
    ]
    
    for idx, title, row in examples:
        # 3D visualization
        ax_3d = fig.add_subplot(gs[row, 0], projection='3d')
        
        # Origin
        ax_3d.scatter([0], [0], [0], c='black', s=100, marker='o', label='Origin')
        
        # True direction
        ax_3d.quiver(0, 0, 0, true[idx, 0], true[idx, 1], true[idx, 2],
                    color='blue', arrow_length_ratio=0.3, linewidth=3, 
                    label='True', alpha=0.8)
        
        # Predicted direction
        ax_3d.quiver(0, 0, 0, pred[idx, 0], pred[idx, 1], pred[idx, 2],
                    color='red', arrow_length_ratio=0.3, linewidth=3,
                    label='Predicted', alpha=0.8)
        
        ax_3d.set_xlabel('X', fontweight='bold')
        ax_3d.set_ylabel('Y', fontweight='bold')
        ax_3d.set_zlabel('Z', fontweight='bold')
        ax_3d.set_title(f'{title}\nError: {errors[idx]:.2f}Â°', fontsize=11, fontweight='bold')
        ax_3d.legend(fontsize=8)
        
        # Set equal aspect ratio
        max_range = 1.0
        ax_3d.set_xlim([-max_range, max_range])
        ax_3d.set_ylim([-max_range, max_range])
        ax_3d.set_zlim([-max_range, max_range])
        
        # Component comparison bar chart
        ax_bar = fig.add_subplot(gs[row, 1])
        components = ['X', 'Y', 'Z']
        x_pos = np.arange(len(components))
        width = 0.35
        
        ax_bar.bar(x_pos - width/2, true[idx], width, label='True', alpha=0.8, color='blue')
        ax_bar.bar(x_pos + width/2, pred[idx], width, label='Predicted', alpha=0.8, color='red')
        ax_bar.set_xticks(x_pos)
        ax_bar.set_xticklabels(components)
        ax_bar.set_ylabel('Component Value', fontweight='bold')
        ax_bar.set_title('Component Comparison', fontsize=11, fontweight='bold')
        ax_bar.legend(fontsize=8)
        ax_bar.grid(axis='y', alpha=0.3)
        ax_bar.axhline(0, color='black', linewidth=0.5)
        ax_bar.set_ylim([-1.2, 1.2])
        
        # Info text
        ax_info = fig.add_subplot(gs[row, 2])
        ax_info.axis('off')
        
        cosine = np.sum(pred[idx] * true[idx])
        info_text = f"""DETAILS

Ang Err: {errors[idx]:.2f}Â°
Cosine:  {cosine:.4f}

True Direction:
  X: {true[idx, 0]:+.3f}
  Y: {true[idx, 1]:+.3f}
  Z: {true[idx, 2]:+.3f}

Predicted:
  X: {pred[idx, 0]:+.3f}
  Y: {pred[idx, 1]:+.3f}
  Z: {pred[idx, 2]:+.3f}

Residuals:
  Î”X: {pred[idx, 0] - true[idx, 0]:+.3f}
  Î”Y: {pred[idx, 1] - true[idx, 1]:+.3f}
  Î”Z: {pred[idx, 2] - true[idx, 2]:+.3f}"""
        
        ax_info.text(0.05, 0.95, info_text, transform=ax_info.transAxes, fontsize=8.5,
                    verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8, pad=0.5))


def plot_angular_error_vs_true_angles(predictions, fig):
    """Page 7: Angular error dependence on true theta and phi angles."""
    true = predictions['true_directions']
    errors = predictions['angular_errors']
    
    # Convert to spherical coordinates
    # theta: polar angle (0 to 180Â°)
    # phi: azimuthal angle (-180 to 180Â°)
    theta_true = np.degrees(np.arccos(np.clip(true[:, 2], -1, 1)))
    phi_true = np.degrees(np.arctan2(true[:, 1], true[:, 0]))
    
    gs = gridspec.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)
    
    # Plot 1: 2D histogram - Error vs theta
    ax1 = fig.add_subplot(gs[0, 0])
    
    # Create 2D histogram: bins for theta (x-axis) and error (y-axis)
    theta_bins = np.linspace(0, 180, 50)
    error_bins = np.linspace(0, np.percentile(errors, 99), 50)
    
    h1 = ax1.hist2d(theta_true, errors, bins=[theta_bins, error_bins], 
                    cmap='YlOrRd', cmin=1)
    cbar1 = plt.colorbar(h1[3], ax=ax1, label='Count')
    
    ax1.set_xlabel('True Î¸ (degrees)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_title('Angular Error vs True Î¸', fontsize=14, fontweight='bold')
    ax1.grid(alpha=0.3, linestyle=':', linewidth=0.5)
    
    # Plot 2: 2D histogram - Error vs phi
    ax2 = fig.add_subplot(gs[0, 1])
    
    phi_bins = np.linspace(-180, 180, 50)
    
    h2 = ax2.hist2d(phi_true, errors, bins=[phi_bins, error_bins], 
                    cmap='YlOrRd', cmin=1)
    cbar2 = plt.colorbar(h2[3], ax=ax2, label='Count')
    
    ax2.set_xlabel('True Ï† (degrees)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Angular Error (degrees)', fontsize=12, fontweight='bold')
    ax2.set_title('Angular Error vs True Ï†', fontsize=14, fontweight='bold')
    ax2.grid(alpha=0.3, linestyle=':', linewidth=0.5)
    
    # Plot 3: 2D histogram - mean error in theta-phi space
    ax3 = fig.add_subplot(gs[1, :])
    
    # Calculate mean error per bin
    theta_bins_3 = np.linspace(0, 180, 45)
    phi_bins_3 = np.linspace(-180, 180, 45)
    
    # Use binned_statistic_2d to compute mean
    from scipy.stats import binned_statistic_2d, gaussian_kde
    mean_errors, theta_edges, phi_edges, _ = binned_statistic_2d(
        theta_true, phi_true, errors, statistic='mean', 
        bins=[theta_bins_3, phi_bins_3])
    
    # Plot as image
    im = ax3.imshow(mean_errors.T, origin='lower', cmap='hot', aspect='auto',
                   extent=[0, 180, -180, 180], vmin=0, vmax=np.percentile(errors, 90))
    cbar3 = plt.colorbar(im, ax=ax3, label='Mean Angular Error (Â°)')
    
    ax3.set_xlabel('True Î¸ (degrees)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('True Ï† (degrees)', fontsize=12, fontweight='bold')
    ax3.set_title('Mean Angular Error in (Î¸, Ï†) Space', fontsize=14, fontweight='bold')




def generate_cosine_energy_pdf(predictions, results_dir, energy_bins=None):
    """
    Generate 2D probability density function of cosine similarity vs energy.
    
    This creates a likelihood function P(cosine | energy) that can be used for
    minimization procedures.
    
    Parameters:
    -----------
    predictions : dict
        Predictions dictionary with cosine_similarity and energies
    results_dir : Path
        Directory to save the PDF file
    energy_bins : list of tuples, optional
        Energy bin edges. Default: [(40, 50), (50, 70)]
    
    Returns:
    --------
    pdf_data : dict
        Dictionary containing the 2D PDF, bin edges, and metadata
    """
    if energy_bins is None:
        energy_bins = [(40, 50), (50, 70)]  # Default bins in MeV
    
    cosine_sim = predictions['cosine_similarity']
    energies = predictions['energies']
    
    # Filter valid energies
    valid_mask = (energies > 0) & (energies < 1000)
    cosine_valid = cosine_sim[valid_mask]
    energies_valid = energies[valid_mask]
    
    # Define cosine bins (fine binning for likelihood)
    n_cosine_bins = 100
    cosine_bin_edges = np.linspace(-1, 1, n_cosine_bins + 1)
    cosine_bin_centers = (cosine_bin_edges[:-1] + cosine_bin_edges[1:]) / 2
    
    # Initialize 2D PDF array
    n_energy_bins = len(energy_bins)
    pdf_2d = np.zeros((n_energy_bins, n_cosine_bins))
    
    # Fill PDF for each energy bin with KDE smoothing
    from scipy.stats import gaussian_kde
    
    for i, (e_min, e_max) in enumerate(energy_bins):
        # Select events in this energy bin
        energy_mask = (energies_valid >= e_min) & (energies_valid < e_max)
        cosine_in_bin = cosine_valid[energy_mask]
        n_events = len(cosine_in_bin)
        
        if n_events > 0:
            if n_events >= 100:
                # Use KDE smoothing for better PDF estimation
                # Bandwidth selection: Scott's rule with adjustment for low stats
                bw_factor = 'scott' if n_events >= 500 else 0.8 * (n_events / 500) ** 0.2
                try:
                    kde = gaussian_kde(cosine_in_bin, bw_method=bw_factor)
                    pdf_2d[i, :] = kde(cosine_bin_centers)
                    
                    # Ensure PDF is normalized (numerical integration)
                    bin_width = cosine_bin_edges[1] - cosine_bin_edges[0]
                    integral = np.sum(pdf_2d[i, :]) * bin_width
                    if integral > 0:
                        pdf_2d[i, :] /= integral
                except:
                    # Fallback to histogram if KDE fails
                    counts, _ = np.histogram(cosine_in_bin, bins=cosine_bin_edges)
                    bin_width = cosine_bin_edges[1] - cosine_bin_edges[0]
                    pdf_2d[i, :] = counts / (counts.sum() * bin_width)
            else:
                # For very low statistics (<100), use histogram
                counts, _ = np.histogram(cosine_in_bin, bins=cosine_bin_edges)
                bin_width = cosine_bin_edges[1] - cosine_bin_edges[0]
                pdf_2d[i, :] = counts / (counts.sum() * bin_width)
                print(f"âš ï¸  Low statistics in bin [{e_min}, {e_max}] MeV: {n_events} events (using histogram)")
        else:
            print(f"Warning: No events in energy bin [{e_min}, {e_max}] MeV")
    
    # Prepare output data
    pdf_data = {
        'pdf_2d': pdf_2d,  # Shape: (n_energy_bins, n_cosine_bins) - smoothed with KDE
        'cosine_bin_edges': cosine_bin_edges,
        'cosine_bin_centers': cosine_bin_centers,
        'energy_bins': energy_bins,
        'n_events_per_bin': [np.sum((energies_valid >= e[0]) & (energies_valid < e[1])) 
                             for e in energy_bins],
        'smoothing_method': 'KDE (Gaussian kernel) for N>=100, histogram for N<100'
    }
    
    # Save to file
    output_path = Path(results_dir) / 'cosine_energy_pdf.npz'
    np.savez(output_path, **pdf_data)
    print(f"\nâœ… 2D PDF saved to: {output_path}")
    print(f"   Shape: {pdf_2d.shape} (energy_bins Ã— cosine_bins)")
    print(f"   Energy bins: {energy_bins}")
    print(f"   Cosine bins: {n_cosine_bins} bins from -1 to 1")
    print(f"   Events per energy bin: {pdf_data['n_events_per_bin']}")
    
    # Create visualization
    visualize_cosine_energy_pdf(pdf_data, results_dir)
    
    return pdf_data, output_path


def visualize_cosine_energy_pdf(pdf_data, results_dir):
    """Create a visualization of the 2D cosine-energy PDF."""
    pdf_2d = pdf_data['pdf_2d']
    cosine_centers = pdf_data['cosine_bin_centers']
    energy_bins = pdf_data['energy_bins']
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: 2D heatmap
    ax1 = axes[0]
    
    # Create custom y-tick labels for energy bins
    energy_labels = [f"{e[0]}-{e[1]} MeV" for e in energy_bins]
    
    im = ax1.imshow(pdf_2d, aspect='auto', cmap='hot', origin='lower',
                   extent=[-1, 1, 0, len(energy_bins)])
    
    ax1.set_xlabel('Cosine Similarity', fontsize=13, fontweight='bold')
    ax1.set_ylabel('Energy Bin', fontsize=13, fontweight='bold')
    ax1.set_title('2D PDF: P(cosine | energy)', fontsize=14, fontweight='bold')
    ax1.set_yticks(np.arange(len(energy_bins)) + 0.5)
    ax1.set_yticklabels(energy_labels)
    
    cbar = plt.colorbar(im, ax=ax1)
    cbar.set_label('Probability Density', rotation=270, labelpad=20, 
                   fontsize=12, fontweight='bold')
    
    # Plot 2: PDF curves for each energy bin
    ax2 = axes[1]
    
    colors = plt.cm.viridis(np.linspace(0, 1, len(energy_bins)))
    for i, (e_bin, color) in enumerate(zip(energy_bins, colors)):
        label = f"{e_bin[0]}-{e_bin[1]} MeV (N={pdf_data['n_events_per_bin'][i]})"
        ax2.plot(cosine_centers, pdf_2d[i, :], linewidth=2, label=label, color=color)
    
    ax2.set_xlabel('Cosine Similarity', fontsize=13, fontweight='bold')
    ax2.set_ylabel('Probability Density', fontsize=13, fontweight='bold')
    ax2.set_title('PDF Curves per Energy Bin', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=9, loc='upper left', ncol=2, framealpha=0.9)
    ax2.grid(alpha=0.3)
    ax2.set_xlim([-1, 1])
    
    plt.tight_layout()
    
    # Save visualization
    output_path = Path(results_dir) / 'cosine_energy_pdf_visualization.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Visualization saved to: {output_path}")

def generate_comprehensive_analysis(results_dir, output_pdf=None):
    """Generate comprehensive multi-page PDF analysis."""
    
    print("\n" + "="*80)
    print("COMPREHENSIVE ELECTRON DIRECTION ANALYSIS")
    print("="*80 + "\n")
    
    # Load results
    print("ðŸ“Š Loading results...")
    results, predictions = load_results(results_dir)
    
    if predictions is None:
        print("âŒ No predictions found. Run training with save_predictions=True")
        return
    
    model_name = results['config'].get('model_name', results['config'].get('model', {}).get('name', 'three_plane_model'))
    print(f"âœ“ Model: {model_name}")
    print(f"âœ“ Predictions: {len(predictions['angular_errors']):,} samples")
    
    # Determine output path
    if output_pdf is None:
        output_pdf = Path(results_dir) / f"{model_name}_comprehensive_analysis.pdf"
    else:
        output_pdf = Path(output_pdf)
    
    print(f"âœ“ Output: {output_pdf}\n")
    
    # Create multi-page PDF
    with PdfPages(output_pdf) as pdf:
        # Page 1: Angular error analysis
        print("ðŸ“ˆ Generating page 1/7: Angular Error Analysis...")
        fig = plt.figure(figsize=(14, 10))
        plot_angular_error_analysis(predictions, results, fig)
        fig.suptitle(f'{model_name.upper()} - Angular Error Analysis', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 2: Training history
        print("ðŸ“ˆ Generating page 2/7: Training History...")
        fig = plt.figure(figsize=(14, 10))
        plot_training_history(results, fig)
        fig.suptitle(f'{model_name.upper()} - Training History', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 3: Component correlations
        print("ðŸ“ˆ Generating page 3/7: Component Correlations...")
        fig = plt.figure(figsize=(16, 10))
        plot_component_correlations(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Component Correlations', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 4: Energy analysis
        print("ðŸ“ˆ Generating page 4/7: Energy-Dependent Performance...")
        fig = plt.figure(figsize=(14, 10))
        plot_energy_analysis(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Energy-Dependent Performance', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 5: Cosine similarity
        print("ðŸ“ˆ Generating page 5/7: Cosine Similarity Analysis...")
        fig = plt.figure(figsize=(16, 10))
        plot_cosine_similarity_analysis(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Cosine Similarity Analysis', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 6: Best/worst predictions
        print("ðŸ“ˆ Generating page 6/7: Best and Worst Predictions...")
        fig = plt.figure(figsize=(16, 12))
        plot_best_worst_predictions(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Best and Worst Predictions', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
        # Page 7: Angular error vs true angles
        print("ðŸ“ˆ Generating page 7/7: Error vs True Angles...")
        fig = plt.figure(figsize=(14, 10))
        plot_angular_error_vs_true_angles(predictions, fig)
        fig.suptitle(f'{model_name.upper()} - Error Dependence on True Angles', 
                    fontsize=16, fontweight='bold', y=0.995)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)
        
    # Generate 2D cosine-energy PDF for likelihood analysis
    print("\nðŸ“Š Generating 2D cosine-energy PDF...")
    if 'energies' in predictions and 'cosine_similarity' in predictions:
        # Custom energy bins: 2 MeV steps until 30, 5 MeV steps until 40, then 40-50, 50-70
        # Starting from 2 MeV (cut applied at lower energies)
        energy_bins = [
            (2, 4), (4, 6), (6, 8), (8, 10),
            (10, 12), (12, 14), (14, 16), (16, 18), (18, 20),
            (20, 22), (22, 24), (24, 26), (26, 28), (28, 30),
            (30, 35), (35, 40), (40, 50), (50, 70)
        ]
        pdf_data, pdf_path = generate_cosine_energy_pdf(predictions, Path(results_dir), energy_bins=energy_bins)
    else:
        print("âš ï¸  Energy or cosine data not available, skipping PDF generation")
    
    
    print(f"\nâœ… Analysis complete! Saved to: {output_pdf}")
    print("="*80 + "\n")
    
    return output_pdf


def main():
    parser = argparse.ArgumentParser(
        description='Generate comprehensive ED analysis PDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s /path/to/results/dir
  %(prog)s /path/to/results/dir -o custom_name.pdf
        """
    )
    parser.add_argument('results_dir', help='Path to model results directory')
    parser.add_argument('-o', '--output', help='Output PDF path (optional)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.results_dir):
        print(f"âŒ Directory not found: {args.results_dir}")
        return 1
    
    try:
        generate_comprehensive_analysis(args.results_dir, args.output)
        return 0
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
