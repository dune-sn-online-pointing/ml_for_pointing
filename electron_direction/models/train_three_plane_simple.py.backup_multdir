"""
Simple 3-plane electron direction training with matched data.
Uses fixed hyperparameters for initial testing.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'python'))

import json
import argparse
import numpy as np
from datetime import datetime
from pathlib import Path

# Import custom modules
import data_loader
from three_plane_cnn import build_three_plane_cnn

def main():
    parser = argparse.ArgumentParser(description='Train 3-plane electron direction CNN (simple)')
    parser.add_argument('-j', '--json', type=str, required=True,
                       help='JSON configuration file')
    args = parser.parse_args()
    
    # Load configuration
    with open(args.json, 'r') as f:
        config = json.load(f)
    
    print("=" * 70)
    print("THREE-PLANE ELECTRON DIRECTION TRAINING (SIMPLE)")
    print("=" * 70)
    print(f"Config: {args.json}")
    print(f"Version: {config['model'].get('name', 'unknown')}")
    print()
    
    # Extract config parameters
    data_config = config['data']
    model_config = config['model']
    training_config = config['training']
    output_config = config['output']
    
    # Data directory
    data_dir = data_config['data_directories'][0]
    max_samples = data_config.get('max_samples', None)
    train_split = data_config.get('train_split', 0.8)
    shuffle = data_config.get('shuffle', True)
    
    # Model parameters
    input_shape = tuple(model_config['input_shape'])
    output_dim = model_config['output_dim']
    normalize_output = model_config.get('normalize_output', True)
    
    # Training parameters
    loss = training_config['loss']
    epochs = training_config.get('epochs', 100)
    batch_size = training_config.get('batch_size', 32)
    learning_rate = training_config.get('learning_rate', 1e-3)
    
    # Model hyperparameters - read from config or use defaults
    model_config = config.get('model', {})
    n_conv_layers = model_config.get('n_conv_layers', 3)
    n_filters = model_config.get('n_filters', 32)
    kernel_size = model_config.get('kernel_size', 3)
    n_dense_layers = model_config.get('n_dense_layers', 2)
    n_dense_units = model_config.get('n_dense_units', 256)
    
    print("=" * 70)
    print("STEP 1: LOADING DATA")
    print("=" * 70)
    print(f"Data directory: {data_dir}")
    print(f"Max samples: {max_samples}")
    print()
    
    # Load 3-plane matched data
    images_u, images_v, images_x, metadata = data_loader.load_three_plane_matched(
        data_dir=data_dir,
        max_samples=max_samples,
        shuffle=shuffle,
        verbose=True
    )
    
    # Extract direction labels from metadata
    directions = data_loader.extract_direction_labels(metadata)
    
    print(f"\nData shapes:")
    print(f"  U images: {images_u.shape}")
    print(f"  V images: {images_v.shape}")
    print(f"  X images: {images_x.shape}")
    print(f"  Directions: {directions.shape}")
    
    # Add channel dimension if needed
    if len(images_u.shape) == 3:
        images_u = np.expand_dims(images_u, axis=-1)
        images_v = np.expand_dims(images_v, axis=-1)
        images_x = np.expand_dims(images_x, axis=-1)
        print(f"\nAdded channel dimension:")
        print(f"  New shapes: {images_u.shape}, {images_v.shape}, {images_x.shape}")
    
    # Split into train/val
    n_train = int(len(images_u) * train_split)
    
    train_u = images_u[:n_train]
    train_v = images_v[:n_train]
    train_x = images_x[:n_train]
    train_y = directions[:n_train]
    train_metadata = metadata[:n_train]
    
    val_u = images_u[n_train:]
    val_v = images_v[n_train:]
    val_x = images_x[n_train:]
    val_y = directions[n_train:]
    val_metadata = metadata[n_train:]
    
    print(f"\nTrain/Val split:")
    print(f"  Train: {len(train_u)} samples")
    print(f"  Val:   {len(val_u)} samples")
    
    print("\n" + "=" * 70)
    print("STEP 2: BUILDING MODEL")
    print("=" * 70)
    print(f"Architecture: 3-plane CNN with branches")
    print(f"  Conv layers per branch: {n_conv_layers}")
    print(f"  Filters: {n_filters}")
    print(f"  Kernel size: {kernel_size}")
    print(f"  Dense layers: {n_dense_layers}")
    print(f"  Dense units: {n_dense_units}")
    print(f"  Loss: {loss}")
    print(f"  Learning rate: {learning_rate}")
    print()
    
    # Build model
    model = build_three_plane_cnn(
        input_shape=input_shape,
        output_dim=output_dim,
        n_conv_layers=n_conv_layers,
        n_filters=n_filters,
        kernel_size=kernel_size,
        n_dense_layers=n_dense_layers,
        n_dense_units=n_dense_units,
        learning_rate=learning_rate
    )
    
    # Setup custom loss if needed
    import tensorflow as tf
    from tensorflow import keras
    
    if loss == 'cosine_similarity':
        loss_fn = keras.losses.CosineSimilarity(axis=1)
    elif loss == 'angular_loss':
        # Import custom loss
        sys.path.insert(0, os.path.dirname(__file__))
        from direction_losses import angular_loss
        loss_fn = angular_loss
    elif loss == 'focal_angular_loss':
        from direction_losses import focal_angular_loss
        loss_fn = focal_angular_loss
    elif loss == 'hybrid_loss':
        from direction_losses import hybrid_loss
        loss_fn = hybrid_loss
    else:
        raise ValueError(f"Unknown loss: {loss}")
    
    # Recompile with the correct loss
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate, clipnorm=0.5),
        loss=loss_fn,
        metrics=['mae']
    )
    
    print(f"✓ Model built and compiled with {loss}")
    model.summary()
    
    print("\n" + "=" * 70)
    print("STEP 3: TRAINING")
    print("=" * 70)
    
    # Setup callbacks
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    
    # Create output directory
    version = config['model'].get('name', 'unknown')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(output_config['base_dir']) / f"three_plane_{version}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_dir = output_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    print(f"Output directory: {output_dir}")
    print()
    
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=training_config.get('early_stopping_patience', 15),
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=training_config.get('reduce_lr_factor', 0.5),
            patience=training_config.get('reduce_lr_patience', 5),
            min_lr=training_config.get('min_lr', 1e-6),
            verbose=1
        ),
        ModelCheckpoint(
            filepath=str(checkpoint_dir / "model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.keras"),
            monitor='val_loss',
            save_best_only=output_config.get('save_best_only', True),
            verbose=1
        )
    ]
    
    # Train
    history = model.fit(
        [train_u, train_v, train_x],
        train_y,
        validation_data=([val_u, val_v, val_x], val_y),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    print("\n" + "=" * 70)
    print("STEP 4: EVALUATION")
    print("=" * 70)
    
    # Evaluate on validation set
    predictions = model.predict([val_u, val_v, val_x])
    
    # Normalize predictions
    pred_norms = np.linalg.norm(predictions, axis=1, keepdims=True)
    predictions = predictions / (pred_norms + 1e-8)
    
    # Calculate angular errors
    dot_products = np.sum(predictions * val_y, axis=1)
    dot_products = np.clip(dot_products, -1.0, 1.0)
    angular_errors = np.arccos(dot_products) * 180.0 / np.pi
    
    print(f"\nAngular Error Statistics:")
    print(f"  Mean:   {np.mean(angular_errors):.2f}°")
    print(f"  Median: {np.median(angular_errors):.2f}°")
    print(f"  Std:    {np.std(angular_errors):.2f}°")
    print(f"  25th:   {np.percentile(angular_errors, 25):.2f}°")
    print(f"  75th:   {np.percentile(angular_errors, 75):.2f}°")
    
    # Extract energy from validation metadata for likelihood building
    # Column 10 contains true_particle_energy in MeV
    offset = 1 if val_metadata.shape[1] == 12 else 0
    val_energies = val_metadata[:, 10 + offset].astype(np.float32)
    
    # Save predictions for later analysis (cosine plots, likelihood, etc.)
    predictions_file = output_dir / "val_predictions.npz"
    np.savez(predictions_file,
             predictions=predictions,
             true_directions=val_y,
             angular_errors=angular_errors,
             energies=val_energies,  # Energy in MeV for likelihood building
             metadata=val_metadata)  # Full metadata for additional analysis
    print(f"\n✓ Predictions saved to: {predictions_file}")
    print(f"  Included: predictions, true_directions, angular_errors, energies, metadata")
    
    # Save results
    results = {
        'config': config,
        'angular_error_mean': float(np.mean(angular_errors)),
        'angular_error_median': float(np.median(angular_errors)),
        'angular_error_std': float(np.std(angular_errors)),
        'angular_error_25th': float(np.percentile(angular_errors, 25)),
        'angular_error_75th': float(np.percentile(angular_errors, 75)),
        'history': {k: [float(v) for v in history.history[k]] for k in history.history.keys()}
    }
    
    results_file = output_dir / "results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n✓ Results saved to: {results_file}")
    print(f"✓ Best angular error: {np.mean(angular_errors):.2f}° (mean), {np.median(angular_errors):.2f}° (median)")
    print()

if __name__ == '__main__':
    main()
