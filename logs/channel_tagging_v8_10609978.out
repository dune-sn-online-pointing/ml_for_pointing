=========================================
HTCondor Channel Tagging Training
=========================================
Job started: Thu Nov  6 01:33:05 PM CET 2025
Host: b9p19p3621.cern.ch
Plane: X
Max samples: ''
Config: json/channel_tagging/production_v8_streaming.json
=========================================
LCG environment: LCG_106_cuda/x86_64-el9-gcc11-opt
Python: /cvmfs/sft.cern.ch/lcg/views/LCG_106_cuda/x86_64-el9-gcc11-opt/bin/python3 (Python 3.11.9)
No GPU
Running: ./scripts/train_channel_tagging.sh -j json/channel_tagging/production_v8_streaming.json --plane X
=========================================
âœ“ Sourced LCG environment: LCG_106_cuda/x86_64-el9-gcc11-opt
âœ“ Added local packages to PYTHONPATH
[0;34m[INFO][0m ML for Pointing Environment
[0;34m[INFO][0m Repository: /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing
[0;34m[INFO][0m Python modules: /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/python
[0;34m[INFO][0m Data directory: /eos/home-e/evilla/dune/sn-tps/images_test
[0;34m[INFO][0m Output directory: /eos/user/e/evilla/dune/sn-tps/neural_networks
[0;34m[INFO][0m Python version: 3.11.9
[0;32m[SUCCESS][0m TensorFlow 2.16.1 is available
Running: python3 channel_tagging/ct_training.py --input_json json/channel_tagging/production_v8_streaming.json --plane X

======================================================================
CHANNEL TAGGING TRAINING
======================================================================

âš  GPU is NOT available - training will use CPU

Loading configuration from: json/channel_tagging/production_v8_streaming.json
Configuration loaded successfully

Output directory: /eos/user/e/evilla/dune/sn-tps/neural_networks/mt_identifier/hyperopt_simple_cnn/plane_X/20251106_133415
Configuration saved to: /eos/user/e/evilla/dune/sn-tps/neural_networks/mt_identifier/hyperopt_simple_cnn/plane_X/20251106_133415/config.json
Arguments saved to: /eos/user/e/evilla/dune/sn-tps/neural_networks/mt_identifier/hyperopt_simple_cnn/plane_X/20251106_133415/args.json
Selected model: hyperopt_simple_cnn

======================================================================
STEP 1: DATA PREPARATION
======================================================================

======================================================================
USING STREAMING DATA LOADER (generator-based)
======================================================================

============================================================
STREAMING DATA LOADER
============================================================
Found 3746 files
Plane: X
Batch size: 32

âœ“ Detected NEW dict metadata format

Counting samples...
  Processed 100/3746 files...
  Processed 200/3746 files...
  Processed 300/3746 files...
  Processed 400/3746 files...
  Processed 500/3746 files...
  Processed 600/3746 files...
  Processed 700/3746 files...
  Processed 800/3746 files...
  Processed 900/3746 files...
  Processed 1000/3746 files...
  Processed 1100/3746 files...
  Processed 1200/3746 files...
  Processed 1300/3746 files...
  Processed 1400/3746 files...
  Processed 1500/3746 files...
  Processed 1600/3746 files...
  Processed 1700/3746 files...
  Processed 1800/3746 files...
  Processed 1900/3746 files...
  Processed 2000/3746 files...
  Processed 2100/3746 files...
  Processed 2200/3746 files...
  Processed 2300/3746 files...
  Processed 2400/3746 files...
  Processed 2500/3746 files...
  Processed 2600/3746 files...
  Processed 2700/3746 files...
  Processed 2800/3746 files...
  Processed 2900/3746 files...
  Processed 3000/3746 files...
  Processed 3100/3746 files...
  Processed 3200/3746 files...
  Processed 3300/3746 files...
  Processed 3400/3746 files...
  Processed 3500/3746 files...
  Processed 3600/3746 files...
  Processed 3700/3746 files...

Total samples: 173467
Class distribution: {0: 0, 1: 173467}
Limiting to 100000 samples

Split sizes:
  Train: 70000
  Val: 15000
  Test: 15000

âœ“ Streaming datasets created successfully
============================================================


======================================================================
STEP 2: MODEL TRAINING
======================================================================
Running the hyperparameter search...
  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]                                                      (567,)
  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]


âœ— Error during training: Unrecognized data type: x=(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 208, 1242, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>, None) (of type <class 'tuple'>)
