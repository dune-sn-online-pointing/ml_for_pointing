#!/usr/bin/env python3
"""
MT Identifier Training with Incremental Batch Learning
FIXED VERSION - Uses metadata[:, 1] (is_main_track) for labels, NOT ES/CC paths!
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import json
import sys
from pathlib import Path
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import glob


def create_mt_model(input_shape=(128, 32, 1), n_filters=64, n_conv_layers=4,
                    n_dense_layers=2, n_dense_units=256, dropout_rate=0.3):
    """Create CNN model for MT classification."""
    
    inputs = keras.layers.Input(shape=input_shape)
    x = inputs
    
    # Convolutional layers
    for i in range(n_conv_layers):
        filters = n_filters
        x = keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)
        x = keras.layers.MaxPooling2D((2, 2))(x)
        x = keras.layers.Dropout(dropout_rate / 2)(x)
    
    # Flatten
    x = keras.layers.Flatten()(x)
    
    # Dense layers
    for _ in range(n_dense_layers):
        x = keras.layers.Dense(n_dense_units, activation='relu')(x)
        x = keras.layers.Dropout(dropout_rate)(x)
    
    # Output: binary classification
    output = keras.layers.Dense(1, activation='sigmoid', name='mt_output')(x)
    
    model = keras.Model(inputs=inputs, outputs=output)
    
    return model

def load_incremental_batch(all_files, batch_size, max_samples_per_file=None):
    """
    Load a balanced batch of main track (1) and non-main track (0) samples.
    Uses metadata[:, 1] to determine is_main_track label.
    
    Args:
        all_files: List of NPZ file paths
        batch_size: Number of samples per class
        max_samples_per_file: Max samples to load from each file (None = all)
    
    Returns:
        images, labels (both numpy arrays)
    """
    print(f"Loading {batch_size} samples per class...")
    
    images_list = []
    labels_list = []
    
    mt_count = 0  # Main track count
    nonmt_count = 0  # Non-main track count
    
    # Shuffle files for this batch
    file_order = np.random.permutation(len(all_files))
    
    for file_idx in file_order:
        if mt_count >= batch_size and nonmt_count >= batch_size:
            break
            
        file_path = all_files[file_idx]
        
        try:
            data = np.load(file_path)
            img = data['images']
            meta = data['metadata']  # Shape: (N, 15)
            
            # Extract is_main_track from metadata column 1
            is_main_track = meta[:, 1].astype(bool)  # Index [1] = is_main_track
            
            # Handle 2D images
            if img.ndim == 2:
                img = img[np.newaxis, ...]
                is_main_track = np.array([is_main_track])
            
            # Determine how many samples to take from this file
            n_samples = min(len(img), max_samples_per_file) if max_samples_per_file else len(img)
            
            for i in range(n_samples):
                label = int(is_main_track[i])  # 1 = main track, 0 = non-main track
                
                if label == 1 and mt_count < batch_size:
                    images_list.append(img[i])
                    labels_list.append(1)
                    mt_count += 1
                elif label == 0 and nonmt_count < batch_size:
                    images_list.append(img[i])
                    labels_list.append(0)
                    nonmt_count += 1
                
                # Check if both classes are full
                if mt_count >= batch_size and nonmt_count >= batch_size:
                    break
                    
        except Exception as e:
            print(f"Warning: Failed to load {file_path}: {e}")
            continue
    
    images = np.array(images_list)
    labels = np.array(labels_list)
    
    print(f"Loaded: {len(images)} samples ({mt_count} Main Track + {nonmt_count} Non-MT)")
    
    return images, labels


def load_test_set(all_files, test_size, max_samples_per_file=None):
    """
    Load a balanced test set with metadata.
    Uses metadata[:, 1] for is_main_track label.
    
    Returns:
        images, labels, metadata_list
    """
    print("Loading test data...")
    
    images_list = []
    labels_list = []
    metadata_list = []
    
    mt_count = 0
    nonmt_count = 0
    
    # Shuffle for random test set
    file_order = np.random.permutation(len(all_files))
    
    for file_idx in file_order:
        if mt_count >= test_size and nonmt_count >= test_size:
            break
            
        file_path = all_files[file_idx]
        
        try:
            data = np.load(file_path)
            img = data['images']
            meta = data['metadata']  # Shape: (N, 15)
            
            # Extract is_main_track from metadata column 1
            is_main_track = meta[:, 1].astype(bool)
            
            # Handle 2D images
            if img.ndim == 2:
                img = img[np.newaxis, ...]
                meta = meta[np.newaxis, :]
                is_main_track = np.array([is_main_track])
            
            # Get true_nu_energy from metadata column 11
            true_nu_energy = meta[:, 11]
            
            # Determine how many samples to take
            n_samples = min(len(img), max_samples_per_file) if max_samples_per_file else len(img)
            
            for i in range(n_samples):
                label = int(is_main_track[i])
                
                if label == 1 and mt_count < test_size:
                    images_list.append(img[i])
                    labels_list.append(1)
                    # Store metadata
                    metadata_dict = {
                        "run": -1,  # Not available in current format
                        "event": -1,
                        "true_energy_sum": float(true_nu_energy[i])
                    }
                    metadata_list.append(metadata_dict)
                    mt_count += 1
                elif label == 0 and nonmt_count < test_size:
                    images_list.append(img[i])
                    labels_list.append(0)
                    metadata_dict = {
                        "run": -1,
                        "event": -1,
                        "true_energy_sum": float(true_nu_energy[i])
                    }
                    metadata_list.append(metadata_dict)
                    nonmt_count += 1
                
                if mt_count >= test_size and nonmt_count >= test_size:
                    break
                    
        except Exception as e:
            print(f"Warning: Failed to load {file_path}: {e}")
            continue
    
    images = np.array(images_list)
    labels = np.array(labels_list)
    
    print(f"Test set: {len(images)} samples ({mt_count} Main Track + {nonmt_count} Non-MT)")
    
    return images, labels, metadata_list


def main():
    # Load config
    if len(sys.argv) < 2:
        print("Usage: python train_mt_incremental.py <config.json>")
        sys.exit(1)
    
    config_path = Path(sys.argv[1])
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Extract config
    model_config = config.get('model', {})
    dataset_config = config.get('dataset', {})
    training_config = config.get('training', {})
    output_config = config.get('output', {})
    
    # Parameters
    model_name = model_config.get('name', 'simple_cnn')
    plane = dataset_config.get('plane', 'X')
    max_samples = dataset_config.get('max_samples', 50000)
    data_dirs = dataset_config.get('data_directories', [])
    
    epochs = training_config.get('epochs', 150)
    batch_size = training_config.get('batch_size', 64)
    learning_rate = training_config.get('learning_rate', 0.001)
    decay_rate = training_config.get('decay_rate', 0.95)
    
    base_dir = Path(output_config.get('base_dir', '.'))
    version = output_config.get('version', 'v1')
    
    print("=" * 80)
    print("MT IDENTIFIER TRAINING (FIXED - Uses is_main_track from metadata)")
    print("=" * 80)
    print(f"Model: {model_name}")
    print(f"Plane: {plane}")
    print(f"Max samples: {max_samples}")
    print(f"Batch size: {batch_size}")
    print(f"Epochs: {epochs}")
    print(f"Learning rate: {learning_rate}")
    print(f"Data directories: {len(data_dirs)}")
    
    # Gather all files
    print("\nGathering data files...")
    all_files = []
    for data_dir in data_dirs:
        files = glob.glob(str(Path(data_dir) / "*.npz"))
        all_files.extend(files)
        print(f"  {Path(data_dir).name}: {len(files)} files")
    
    print(f"Total files: {len(all_files)}")
    
    if len(all_files) == 0:
        print("ERROR: No data files found!")
        sys.exit(1)
    
    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = base_dir / version / f"mt_incremental_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nOutput directory: {output_dir}")
    
    # Save config
    with open(output_dir / "config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    # Create model
    print("\nBuilding model...")
    model = create_mt_model(input_shape=(128, 32, 1))
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy', keras.metrics.AUC(name='auc')]
    )
    
    model.summary()
    
    # Incremental training
    num_batches = max_samples // (batch_size * 2)  # Each batch has batch_size per class
    epochs_per_batch = epochs // num_batches
    train_batch_size = 32  # Internal training batch size
    
    print(f"\nIncremental Training Setup:")
    print(f"  Total samples target: {max_samples}")
    print(f"  Samples per batch: {batch_size * 2} ({batch_size} per class)")
    print(f"  Number of batches: {num_batches}")
    print(f"  Epochs per batch: {epochs_per_batch}")
    
    all_history = []
    
    for batch_idx in range(num_batches):
        print("\n" + "=" * 80)
        print(f"BATCH {batch_idx + 1}/{num_batches}")
        print("=" * 80)
        
        # Load balanced batch
        images, labels = load_incremental_batch(all_files, batch_size)
        
        # Normalize
        if images.ndim == 3:
            images = images[..., np.newaxis]
        
        # Shuffle
        indices = np.random.permutation(len(images))
        images = images[indices]
        labels = labels[indices]
        
        # Split train/val
        split_idx = int(0.8 * len(images))
        X_train = images[:split_idx]
        y_train = labels[:split_idx]
        X_val = images[split_idx:]
        y_val = labels[split_idx:]
        
        print(f"Training samples: {len(X_train)}")
        print(f"Validation samples: {len(X_val)}")
        
        # Train on this batch
        print(f"\nTraining for {epochs_per_batch} epochs...")
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs_per_batch,
            batch_size=train_batch_size,
            verbose=1
        )
        
        all_history.append(history.history)
        
        # Save model after each batch
        model.save(output_dir / f'model_batch_{batch_idx+1:02d}-{num_batches}.keras')
        print(f"Model saved: model_batch_{batch_idx+1:02d}-{num_batches}.keras")
    
    # Final save
    model.save(output_dir / 'final_model.keras')
    print("\nFinal model saved: final_model.keras")
    
    # Save training history
    np.save(output_dir / "training_history.npy", np.array(all_history, dtype=object))
    
    # EVALUATION
    print("\n" + "=" * 80)
    print("EVALUATING ON TEST SET")
    print("=" * 80)
    
    # Load test set
    test_size = batch_size  # Same size per class as training batches
    test_images, test_labels, test_metadata = load_test_set(all_files, test_size)
    
    # Normalize
    if test_images.ndim == 3:
        test_images = test_images[..., np.newaxis]
    
    # Make predictions
    test_predictions = model.predict(test_images, batch_size=train_batch_size)
    test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()
    
    # Calculate metrics
    accuracy = accuracy_score(test_labels, test_predictions_binary)
    precision = precision_score(test_labels, test_predictions_binary)
    recall = recall_score(test_labels, test_predictions_binary)
    f1 = f1_score(test_labels, test_predictions_binary)
    auc_roc = roc_auc_score(test_labels, test_predictions.flatten())
    
    print("\nTest Set Performance:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1 Score:  {f1:.4f}")
    print(f"  AUC-ROC:   {auc_roc:.4f}")
    
    # Save predictions and labels
    np.save(output_dir / "test_predictions.npy", test_predictions)
    np.save(output_dir / "test_predictions_binary.npy", test_predictions_binary)
    np.save(output_dir / "test_labels.npy", test_labels)
    np.save(output_dir / "test_metadata.npy", np.array(test_metadata, dtype=object))
    
    # Save metrics
    metrics_dir = output_dir / "metrics"
    metrics_dir.mkdir(exist_ok=True)
    
    metrics = {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "auc_roc": float(auc_roc),
        "test_samples": float(len(test_labels))
    }
    
    with open(metrics_dir / "test_metrics.json", 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\nAll results saved to: {output_dir}")
    print("=" * 80)


if __name__ == "__main__":
    main()
