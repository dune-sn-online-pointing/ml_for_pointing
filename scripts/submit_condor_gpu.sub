#!/usr/bin/env condor_submit
# HTCondor submission file for ML training with GPU

# Job configuration
universe                = vanilla
executable              = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/scripts/condor_wrapper.sh
arguments               = $(plane) $(max_samples)

# GPU requirements
request_gpus            = 1
+RequiresGPU            = true
requirements            = (OpSysAndVer =?= "AlmaLinux9")

# Resource requests
request_cpus            = 4
request_memory          = 40GB
request_disk            = 2GB

# Longer time for hyperopt (10 evaluations)
+MaxRuntime             = 43200

# Environment
getenv                  = True
environment             = "PYTHONUNBUFFERED=1"

# Output files with clear job identifier: MT_IDENTIFIER
output                  = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/MT_IDENTIFIER_$(plane)_$(ClusterId).out
error                   = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/MT_IDENTIFIER_$(plane)_$(ClusterId).err
log                     = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/MT_IDENTIFIER_$(plane)_$(ClusterId).log

# Email notification (optional)
# notify_user           = your.email@cern.ch
# notification          = Complete

# File transfer
should_transfer_files   = NO
+JobFlavour             = "tomorrow"

# Submit jobs for each plane
plane                   = X
max_samples             = 
queue 1

# Uncomment to submit for Y and Z planes too
# plane                 = Y
# max_samples           = 
# queue 1

# plane                 = Z
# max_samples           = 
# queue 1
