#!/usr/bin/env condor_submit
# HTCondor submission file for CT v12 (large dataset approach)

# Job configuration
universe                = vanilla
executable              = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/scripts/condor_wrapper_channel_tagging.sh
arguments               = X 100000 json/channel_tagging/production_v12_large_data.json

# GPU requirements
request_gpus            = 1
+RequiresGPU            = true
requirements            = (OpSysAndVer =?= "AlmaLinux9")

# Resource requests
request_cpus            = 4
request_memory          = 8GB
request_disk            = 4GB

# Time limit
+MaxRuntime             = 86400

# Environment
getenv                  = True
environment             = "PYTHONUNBUFFERED=1"

# Output files
output                  = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CT_V12_LARGE_DATA_X_$(ClusterId).out
error                   = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CT_V12_LARGE_DATA_X_$(ClusterId).err
log                     = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CT_V12_LARGE_DATA_X_$(ClusterId).log

# File transfer
should_transfer_files   = NO
+JobFlavour             = "tomorrow"

# Submit
queue 1
