#!/usr/bin/env condor_submit
# HTCondor submission file for Channel Tagging training with GPU

# Job configuration
universe                = vanilla
executable              = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/scripts/condor_wrapper_channel_tagging.sh
arguments               = $(json_config) $(max_samples)

# GPU requirements
request_gpus            = 1
+RequiresGPU            = true
requirements            = (OpSysAndVer =?= "AlmaLinux9")

# Resource requests
request_cpus            = 4
request_memory = 100GB
request_disk            = 4GB

# Longer time for hyperopt (10 evaluations) + larger images
+MaxRuntime             = 86400

# Environment
getenv                  = True
environment             = "PYTHONUNBUFFERED=1"

# Output files with clear job identifier: CHANNEL_TAGGING_100GB
output                  = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CHANNEL_TAGGING_100GB_$(plane)_$(ClusterId).out
error                   = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CHANNEL_TAGGING_100GB_$(plane)_$(ClusterId).err
log                     = /afs/cern.ch/work/e/evilla/private/dune/ml_for_pointing/logs/CHANNEL_TAGGING_100GB_$(plane)_$(ClusterId).log

# Email notification (optional)
# notify_user           = your.email@cern.ch
# notification          = Complete

# File transfer
should_transfer_files   = NO
+JobFlavour             = "tomorrow"

# Submit jobs for each plane
plane                   = X
max_samples             = 
json_config             = json/channel_tagging/production_training.json
queue 1

# Uncomment to submit for Y and Z planes too
# plane                 = Y
# max_samples           = 
# queue 1

# plane                 = Z
# max_samples           = 
# queue 1
